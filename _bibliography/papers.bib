---
---
@inproceedings{ding2023hpc,
author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
title = {HPC-GPT: Integrating Large Language Model for High-Performance Computing},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624172},
doi = {10.1145/3624062.3624172},
abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {951–960},
numpages = {10},
keywords = {Data Race Detection, High-performance Computing, Large Language Model, Neural Network., OpenMP},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{chen2023lm4hpc,
author = {Chen, Le and Lin, Pei-Hung and Vanderbruggen, Tristan and Liao, Chunhua and Emani, Murali and de Supinski, Bronis},
title = {LM4HPC: Towards Effective Language Model Application in&nbsp;High-Performance Computing},
year = {2023},
isbn = {978-3-031-40743-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40744-4_2},
doi = {10.1007/978-3-031-40744-4_2},
abstract = {In recent years, language models (LMs), such as GPT-4, have been widely used in multiple domains, including natural language processing, visualization, and so on. However, applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper, we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets, AI models, and pipelines, our framework is built on top of a range of components from different levels of the machine learning software stack, with Hugging Face-compatible APIs. Using three representative tasks, we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.},
booktitle = {OpenMP: Advanced Task-Based, Device and Compiler Programming: 19th International Workshop on OpenMP, IWOMP 2023, Bristol, UK, September 13–15, 2023, Proceedings},
pages = {18–33},
numpages = {16},
keywords = {High-performance computing, Programming language processing, Language model},
location = {Bristol, United Kingdom}
}

@article{chen2024landscape,
  bibtex_show={true},
  title={The landscape and challenges of HPC research and LLMs},
  author={Chen, Le and Ahmed, Nesreen K and Dutta, Akash and Bhattacharjee, Arijit and Yu, Sixing and Mahmud, Quazi Ishtiaque and Abebe, Waqwoya and Phan, Hung and Sarkar, Aishwarya and Butler, Branden and others},
  journal={arXiv preprint arXiv:2402.02018},
  year={2024}
}

@inproceedings{chen2024ompgpt,
author = {Chen, Le and Bhattacharjee, Arijit and Ahmed, Nesreen and Hasabnis, Niranjan and Oren, Gal and Vo, Vy and Jannesari, Ali},
title = {OMPGPT: A Generative Pre-trained Transformer Model for&nbsp;OpenMP},
year = {2024},
isbn = {978-3-031-69576-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-69577-3_9},
doi = {10.1007/978-3-031-69577-3_9},
abstract = {Large language models (LLMs)such as ChatGPT have significantly advanced the field of Natural Language Processing (NLP). This trend led to the development of code-based large language models such as StarCoder, WizardCoder, and CodeLlama, which are trained extensively on vast repositories of code and programming languages. While the generic abilities of these code LLMs are helpful for many programmers in tasks like code generation, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific model a smarter choice. This paper presents OMPGPT, a novel domain-specific model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we leverage prompt engineering techniques from the NLP domain to create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT’s effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms existing large language models specialized in OpenMP tasks and maintains a notably smaller size, aligning it more closely with the typical hardware constraints of HPC environments. We consider our contribution as a pivotal bridge, connecting the advantage of language models with the specific demands of HPC tasks.},
booktitle = {Euro-Par 2024: Parallel Processing: 30th European Conference on Parallel and Distributed Processing, Madrid, Spain, August 26–30, 2024, Proceedings, Part I},
pages = {121–134},
numpages = {14},
keywords = {Large Language model, OpenMP, HPC},
location = {Madrid, Spain}
}

@inproceedings{chen2023data,
author = {Chen, Le and Ding, Xianzhong and Emani, Murali and Vanderbruggen, Tristan and Lin, Pei-Hung and Liao, Chunhua},
title = {Data Race Detection Using Large Language Models},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624088},
doi = {10.1145/3624062.3624088},
abstract = {Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {215–223},
numpages = {9},
keywords = {OpenMP, data race detection, large language model},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{chen2023learning,
  author       = {Le Chen and
                  Quazi Ishtiaque Mahmud and
                  Hung Phan and
                  Nesreen K. Ahmed and
                  Ali Jannesari},
  editor       = {Dawn Song and
                  Michael Carbin and
                  Tianqi Chen},
  title        = {Learning to Parallelize with OpenMP by Augmented Heterogeneous {AST}
                  Representation},
  booktitle    = {Proceedings of the Sixth Conference on Machine Learning and Systems,
                  MLSys 2023, Miami, FL, USA, June 4-8, 2023},
  publisher    = {mlsys.org},
  year         = {2023},
  url          = {https://proceedings.mlsys.org/paper\_files/paper/2023/hash/8ee477d6175a03d7098fa23641a2d298-Abstract-mlsys2023.html},
  timestamp    = {Fri, 28 Jun 2024 15:58:54 +0200},
  biburl       = {https://dblp.org/rec/conf/mlsys/ChenMPAJ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{tehranijamsaz2023perfograph,
  bibtex_show={true},
  title={Perfograph: A numerical aware program graph representation for performance optimization and program analysis},
  author={TehraniJamsaz, Ali and Mahmud, Quazi Ishtiaque and Chen, Le and Ahmed, Nesreen K and Jannesari, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={57783--57794},
  year={2023}
}

@inproceedings{lei2023creating,
  bibtex_show={true},
  title={Creating a dataset for high-performance computing code translation using LLMS: a bridge between OpenMP Fortran and C++},
  author={Lei, Bin and Ding, Caiwen and Chen, Le and Lin, Pei-Hung and Liao, Chunhua},
  booktitle={2023 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--7},
  year={2023},
  organization={IEEE}
}

@inproceedings{mahmud2025autoparllm,
    title = "{A}uto{P}ar{LLM}: {GNN}-guided Context Generation for Zero-Shot Code Parallelization using {LLM}s",
    author = "Mahmud, Quazi Ishtiaque  and
      TehraniJamsaz, Ali  and
      Phan, Hung D  and
      Chen, Le  and
      Capot{\u{a}}, Mihai  and
      Willke, Theodore L.  and
      Ahmed, Nesreen K.  and
      Jannesari, Ali",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.593/",
    doi = "10.18653/v1/2025.naacl-long.593",
    pages = "11821--11841",
    ISBN = "979-8-89176-189-6",
    abstract = "In-Context Learning (ICL) has been shown to be a powerful technique to augment the capabilities of LLMs for a diverse range of tasks. This work proposes AutoParLLM, a novel way to generate context using guidance from graph neural networks (GNNs) to generate efficient parallel codes. We evaluate AutoParLLM on 12 applications from two well-known benchmark suites of parallel codes: NAS Parallel Benchmark and Rodinia Benchmark. Our results show that AutoParLLM improves the state-of-the-art LLMs (e.g., GPT-4) by 19.9{\%} in NAS and 6.48{\%} in Rodinia benchmark in terms of CodeBERTScore for the task of parallel code generation. Moreover, AutoParLLM improves the ability of the most powerful LLM to date, GPT-4, by achieving 17{\%} (on NAS benchmark) and 16{\%} (on Rodinia benchmark) better speedup. In addition, we propose OMPScore for evaluating the quality of the parallel code and show its effectiveness in evaluating parallel codes."
}

@inproceedings{tehrani2024coderosetta,
  author       = {Ali TehraniJamsaz and
                  Quazi Ishtiaque Mahmud and
                  Le Chen and
                  Nesreen K. Ahmed and
                  Ali Jannesari},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {{PERFOGRAPH:} {A} Numerical Aware Program Graph Representation for
                  Performance Optimization and Program Analysis},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/b41907dd4df5c60f86216b73fe0c7465-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/TehraniJamsazMC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{chen2022multi,
  author={Chen, Le and Mahmud, Quazi Ishtiaque and Jannesari, Ali},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Multi-View Learning for Parallelism Discovery of Sequential Programs}, 
  year={2022},
  volume={},
  number={},
  pages={295-303},
  keywords={Training;Runtime;Dynamics;Semantics;Machine learning;Static analysis;Parallel processing;machine learning;artificial intelligence;parallel program language},
  doi={10.1109/IPDPSW55747.2022.00059}
}

@article{chen2024fortran2cpp,
  bibtex_show={true},
  title={Fortran2cpp: Automating fortran-to-c++ migration using llms via multi-turn dialogue and dual-agent integration},
  author={Chen, Le and Lei, Bin and Zhou, Dunzhi and Lin, Pei-Hung and Liao, Chunhua and Ding, Caiwen and Jannesari, Ali},
  journal={arXiv e-prints},
  pages={arXiv--2412},
  year={2024}
}

@article{chen2023compcodevet,
  bibtex_show={true},
  title={Compcodevet: A compiler-guided validation and enhancement approach for code dataset},
  author={Chen, Le and Bhattacharjee, Arijit and Ahmed, Nesreen K and Hasabnis, Niranjan and Oren, Gal and Lei, Bin and Jannesari, Ali},
  journal={arXiv preprint arXiv:2311.06505},
  year={2023}
}

@article{lei2023flashvideo,
  bibtex_show={true},
  title={Flashvideo: A framework for swift inference in text-to-video generation},
  author={Lei, Bin and Ding, Caiwen and others},
  journal={arXiv preprint arXiv:2401.00869},
  year={2023}
}

@article{chen2023dataracebench,
  title={Dataracebench v1. 4.1 and dataracebench-ml v0. 1: Benchmark suites for data race detection},
  author={Chen, Le and Wu, Wenhao and Siegel, Stephen F and Lin, Pei-Hung and Liao, Chunhua},
  journal={arXiv preprint arXiv:2308.08473},
  year={2023}
}

@INPROCEEDINGS{chen2025pcebench,
  author={Chen, Le and Ahmed, Nesreen and Capotă, Mihai and Willke, Ted and Hasabnis, Niranjan and Jannesari, Ali},
  booktitle={2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={PCEBench: A Multi-Dimensional Benchmark for Evaluating Large Language Models in Parallel Code Generation}, 
  year={2025},
  volume={},
  number={},
  pages={546-557},
  keywords={Technological innovation;Codes;Parallel programming;Large language models;Scalability;Benchmark testing;Software systems;Multitasking;Natural language processing;Synchronization;large language model;parallel code generation;benchmark;evaluation;LLM agent},
  doi={10.1109/IPDPS64566.2025.00055}
}

@inproceedings{Aditya2025first,
author = {Tanikanti, Aditya and C\^{o}t\'{e}, Benoit and Guo, Yanfei and Chen, Le and Saint, Nickolaus and Chard, Ryan and Raffenetti, Ken and Thakur, Rajeev and Uram, Thomas and Foster, Ian and Papka, Michael E. and Vishwanath, Venkatram},
title = {FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access},
year = {2025},
isbn = {9798400718717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731599.3767346},
doi = {10.1145/3731599.3767346},
abstract = {We present the Federated Inference Resource Scheduling Toolkit (FIRST), a framework enabling Inference-as-a-Service across distributed High-Performance Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI models, like Large Language Models (LLMs), on existing HPC infrastructure. Leveraging Globus Auth and Globus Compute, the system allows researchers to run parallel inference workloads via an OpenAI-compliant API on private, secure environments. This cluster-agnostic API allows requests to be distributed across federated clusters, targeting numerous hosted models. FIRST supports multiple inference backends (e.g., vLLM), auto-scales resources, maintains "hot" nodes for low-latency execution, and offers both high-throughput batch and interactive modes. The framework addresses the growing demand for private, secure, and scalable AI inference in scientific workflows, allowing researchers to generate billions of tokens daily on-premises without relying on commercial cloud infrastructure.},
booktitle = {Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {52–60},
numpages = {9},
keywords = {Inference as a Service, High Performance Computing, Job Schedulers, Large Language Models, Globus, Scientific Computing},
location = {
},
series = {SC Workshops '25}
}

@inproceedings{bitan2025unipar,
  bibtex_show={true},
  title={UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC},
  author={Bitan, Tomer and Kadosh, Tal and Kaplan, Erel and Meiri, Shira and Chen, Le and Morales, Peter and Hasabnis, Niranjan and Oren, Gal},
  booktitle={2025 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--9},
  year={2025},
  organization={IEEE}
}

@article{smith2025ai,
  bibtex_show={true},
  title={AI Assistants to Enhance and Exploit the PETSc Knowledge Base},
  author={Smith, Barry and Zhang, Junchao and Zhang, Hong and McInnes, Lois Curfman and Keceli, Murat and Vasan, Archit and Balay, Satish and Isaac, Toby and Chen, Le and Vishwanath, Venkatram},
  journal={arXiv preprint arXiv:2506.20608},
  year={2025}
}